{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computer\n",
    "pip install python-dotenv\n",
    "\n",
    "api_key = os.getenv('YOUTUBE_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab\n",
    "\n",
    "from google.colab import userdata\n",
    "api_key = userdata.get('YOUTUBE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-api-python-client\n",
    "pip install transformers\n",
    "pip install matplotlib\n",
    "pip install wordcloud\n",
    "pip install seaborn\n",
    "pip install nltk\n",
    "pip install isodate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from googleapiclient.discovery import build\n",
    "import isodate\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK veri setlerini indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Çevresel değişkenleri yükle\n",
    "load_dotenv()\n",
    "\n",
    "# Duygu analizi için modeller\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "emotion_analyzer = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "def get_youtube_service():\n",
    "    \"\"\"YouTube API istemcisini başlat.\"\"\"\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Error: YouTube API key not found in environment variables\")\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    return youtube\n",
    "\n",
    "def extract_video_id(youtube_url):\n",
    "    \"\"\"YouTube URL'sinden video ID'sini çıkar.\"\"\"\n",
    "    # Farklı URL formatlarını destekle\n",
    "    regex_patterns = [\n",
    "        r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*',\n",
    "        r'(?:be\\/)([0-9A-Za-z_-]{11}).*',\n",
    "        r'(?:embed\\/)([0-9A-Za-z_-]{11}).*'\n",
    "    ]\n",
    "    for pattern in regex_patterns:\n",
    "        match = re.search(pattern, youtube_url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    raise ValueError(\"Geçersiz YouTube URL'si. Lütfen doğru bir URL girin.\")\n",
    "\n",
    "def get_video_metadata(video_id, youtube):\n",
    "    \"\"\"Belirli bir video hakkında geniş metadata al.\"\"\"\n",
    "    response = youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics,status,topicDetails,liveStreamingDetails\",\n",
    "        id=video_id\n",
    "    ).execute()\n",
    "\n",
    "    if not response['items']:\n",
    "        raise ValueError(\"Bu ID ile eşleşen video bulunamadı.\")\n",
    "    \n",
    "    video_data = response['items'][0]\n",
    "    duration = isodate.parse_duration(video_data['contentDetails']['duration']).total_seconds()\n",
    "    metadata = {\n",
    "        'video_id': video_id,\n",
    "        'title': video_data['snippet']['title'],\n",
    "        'description': video_data['snippet']['description'],\n",
    "        'tags': video_data['snippet'].get('tags', []),\n",
    "        'channel_title': video_data['snippet']['channelTitle'],\n",
    "        'channel_id': video_data['snippet']['channelId'],\n",
    "        'publish_date': video_data['snippet']['publishedAt'],\n",
    "        'thumbnail_url': video_data['snippet']['thumbnails']['high']['url'],\n",
    "        'view_count': int(video_data['statistics'].get('viewCount', 0)),\n",
    "        'like_count': int(video_data['statistics'].get('likeCount', 0)),\n",
    "        'comment_count': int(video_data['statistics'].get('commentCount', 0)),\n",
    "        'duration_seconds': duration,\n",
    "        'licensed_content': video_data['contentDetails'].get('licensedContent', False),\n",
    "        'privacy_status': video_data['status']['privacyStatus'],\n",
    "        'embeddable': video_data['status']['embeddable'],\n",
    "        'public_stats_viewable': video_data['status']['publicStatsViewable'],\n",
    "        'category_id': video_data['snippet'].get('categoryId', None),\n",
    "        'topic_categories': video_data.get('topicDetails', {}).get('topicCategories', []),\n",
    "        'live_broadcast': video_data.get('liveStreamingDetails', {}).get('actualStartTime', None),\n",
    "        'default_language': video_data['snippet'].get('defaultLanguage')\n",
    "    }\n",
    "    return metadata\n",
    "\n",
    "def truncate_text(text, max_tokens=MAX_TOKENS):\n",
    "    \"\"\"Yorum metnini belirli bir token uzunluğuna kısaltır.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Metni ön işleme tabi tutar.\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # URL'leri kaldır\n",
    "    text = re.sub(r'@\\w+', '', text)     # Kullanıcı etiketlerini kaldır\n",
    "    text = re.sub(r'#\\w+', '', text)     # Hashtag'leri kaldır\n",
    "    text = re.sub(r'\\s+', ' ', text)     # Fazla boşlukları kaldır\n",
    "    return text.strip()\n",
    "\n",
    "def get_comment_sentiments(video_id, youtube, max_comments=1000):\n",
    "    \"\"\"Yorumları analiz ederek duygusal dağılım, anahtar kelimeler ve en aktif kullanıcıları bulur.\"\"\"\n",
    "    comments = []\n",
    "    sentiment_counts = Counter()\n",
    "    emotion_counts = Counter()\n",
    "    active_users = Counter()\n",
    "    key_phrases = Counter()\n",
    "    comment_times = []\n",
    "    \n",
    "    stop_words = set(stopwords.words('english') + stopwords.words('turkish'))\n",
    "\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_comments:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=min(max_comments - len(comments), 100),\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            comment_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = comment_snippet['textDisplay']\n",
    "            truncated_text = truncate_text(comment_text)\n",
    "            preprocessed_text = preprocess_text(truncated_text)\n",
    "            author = comment_snippet['authorDisplayName']\n",
    "            like_count = comment_snippet['likeCount']\n",
    "            published_at = comment_snippet['publishedAt']\n",
    "            published_at_dt = datetime.datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            comment_times.append(published_at_dt)\n",
    "\n",
    "            # Duygu analizi\n",
    "            sentiment = sentiment_analyzer(preprocessed_text)[0]\n",
    "            sentiment_counts[sentiment['label']] += 1\n",
    "\n",
    "            # Duygu yoğunluğu\n",
    "            emotions = emotion_analyzer(preprocessed_text)[0]\n",
    "            dominant_emotion = max(emotions, key=lambda x: x['score'])\n",
    "            emotion_counts[dominant_emotion['label']] += 1\n",
    "\n",
    "            # Anahtar kelime analizi\n",
    "            words = word_tokenize(preprocessed_text)\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                if word.isalpha() and word not in stop_words and len(word) > 3:\n",
    "                    key_phrases[word] += 1\n",
    "\n",
    "            # Aktif kullanıcılar\n",
    "            active_users[author] += 1\n",
    "\n",
    "            # Yorum kaydı\n",
    "            comments.append({\n",
    "                'comment_text': comment_text,\n",
    "                'author': author,\n",
    "                'like_count': like_count,\n",
    "                'published_at': published_at,\n",
    "                'sentiment': sentiment['label'],\n",
    "                'emotion': dominant_emotion['label']\n",
    "            })\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    # En sık kullanılan 20 anahtar kelime\n",
    "    top_keywords = key_phrases.most_common(20)\n",
    "    # En aktif 10 kullanıcı\n",
    "    top_active_users = active_users.most_common(10)\n",
    "\n",
    "    return comments, dict(sentiment_counts), dict(emotion_counts), top_keywords, top_active_users, comment_times\n",
    "\n",
    "def generate_wordcloud(key_phrases):\n",
    "    \"\"\"Anahtar kelimelerin kelime bulutunu oluşturur.\"\"\"\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(key_phrases))\n",
    "    plt.figure(figsize=(15, 7.5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('wordcloud.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_sentiment_distribution(sentiment_counts):\n",
    "    \"\"\"Duygu dağılımını görselleştirir.\"\"\"\n",
    "    labels = list(sentiment_counts.keys())\n",
    "    sizes = list(sentiment_counts.values())\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.barplot(x=labels, y=sizes)\n",
    "    plt.title('Duygu Dağılımı')\n",
    "    plt.xlabel('Duygu')\n",
    "    plt.ylabel('Sayı')\n",
    "    plt.savefig('sentiment_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_emotion_distribution(emotion_counts):\n",
    "    \"\"\"Duygu yoğunluğu dağılımını görselleştirir.\"\"\"\n",
    "    labels = list(emotion_counts.keys())\n",
    "    sizes = list(emotion_counts.values())\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=labels, y=sizes)\n",
    "    plt.title('Duygu Yoğunluğu Dağılımı')\n",
    "    plt.xlabel('Duygu')\n",
    "    plt.ylabel('Sayı')\n",
    "    plt.savefig('emotion_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_comment_timeline(comment_times):\n",
    "    \"\"\"Yorumların zaman içindeki dağılımını görselleştirir.\"\"\"\n",
    "    df = pd.DataFrame({'timestamp': comment_times})\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df['count'] = 1\n",
    "    df = df.resample('D').sum()\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    df['count'].plot()\n",
    "    plt.title('Zaman İçinde Yorum Dağılımı')\n",
    "    plt.xlabel('Tarih')\n",
    "    plt.ylabel('Yorum Sayısı')\n",
    "    plt.savefig('comment_timeline.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_metadata_to_json(metadata):\n",
    "    \"\"\"Video metadata bilgilerini JSON dosyasına kaydeder.\"\"\"\n",
    "    with open('video_metadata.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(metadata, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def save_sentiment_analysis_to_json(comments, sentiment_counts, emotion_counts, top_keywords, top_active_users):\n",
    "    \"\"\"Sentiment analiz sonuçlarını JSON dosyasına kaydeder.\"\"\"\n",
    "    data = {\n",
    "        'comments': comments,\n",
    "        'sentiment_distribution': sentiment_counts,\n",
    "        'emotion_distribution': emotion_counts,\n",
    "        'top_keywords': top_keywords,\n",
    "        'top_active_users': top_active_users\n",
    "    }\n",
    "    with open('sentiment_analysis.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    youtube_url = input(\"YouTube Video URL'sini girin: \")\n",
    "    youtube = get_youtube_service()\n",
    "    video_id = extract_video_id(youtube_url)\n",
    "    \n",
    "    # 1. Video Metadata Analizi\n",
    "    metadata = get_video_metadata(video_id, youtube)\n",
    "    save_metadata_to_json(metadata)\n",
    "    print(\"Video metadata bilgileri 'video_metadata.json' dosyasına kaydedildi.\")\n",
    "\n",
    "    # 2. Sentiment Analizi ve Ekstra Analizler\n",
    "    comments, sentiment_counts, emotion_counts, top_keywords, top_active_users, comment_times = get_comment_sentiments(video_id, youtube, max_comments=1000)\n",
    "    save_sentiment_analysis_to_json(comments, sentiment_counts, emotion_counts, top_keywords, top_active_users)\n",
    "    print(\"Sentiment analiz sonuçları 'sentiment_analysis.json' dosyasına kaydedildi.\")\n",
    "\n",
    "    # 3. Görselleştirmeler\n",
    "    generate_wordcloud(top_keywords)\n",
    "    print(\"Kelime bulutu 'wordcloud.png' olarak kaydedildi.\")\n",
    "\n",
    "    plot_sentiment_distribution(sentiment_counts)\n",
    "    print(\"Duygu dağılım grafiği 'sentiment_distribution.png' olarak kaydedildi.\")\n",
    "\n",
    "    plot_emotion_distribution(emotion_counts)\n",
    "    print(\"Duygu yoğunluğu grafiği 'emotion_distribution.png' olarak kaydedildi.\")\n",
    "\n",
    "    plot_comment_timeline(comment_times)\n",
    "    print(\"Yorum zaman çizelgesi 'comment_timeline.png' olarak kaydedildi.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
