{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computer\n",
    "pip install python-dotenv\n",
    "\n",
    "api_key = os.getenv('YOUTUBE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab\n",
    "\n",
    "from google.colab import userdata\n",
    "api_key = userdata.get('YOUTUBE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from googleapiclient.discovery import build\n",
    "import isodate\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "\n",
    "# NLTK veri setlerini indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Çevresel değişkenleri yükle\n",
    "load_dotenv()\n",
    "\n",
    "# Duygu analizi için modeller\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "emotion_analyzer = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "def get_youtube_service():\n",
    "    \"\"\"YouTube API istemcisini başlat.\"\"\"\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Error: YouTube API key not found in environment variables\")\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    return youtube\n",
    "\n",
    "def extract_video_id(youtube_url):\n",
    "    \"\"\"YouTube URL'sinden video ID'sini çıkar.\"\"\"\n",
    "    # Farklı URL formatlarını destekle\n",
    "    regex_patterns = [\n",
    "        r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*',\n",
    "        r'(?:be\\/)([0-9A-Za-z_-]{11}).*',\n",
    "        r'(?:embed\\/)([0-9A-Za-z_-]{11}).*'\n",
    "    ]\n",
    "    for pattern in regex_patterns:\n",
    "        match = re.search(pattern, youtube_url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    raise ValueError(\"Geçersiz YouTube URL'si. Lütfen doğru bir URL girin.\")\n",
    "\n",
    "def get_video_metadata(video_id, youtube):\n",
    "    \"\"\"Belirli bir video hakkında geniş metadata al.\"\"\"\n",
    "    response = youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics,status,topicDetails,liveStreamingDetails\",\n",
    "        id=video_id\n",
    "    ).execute()\n",
    "\n",
    "    if not response['items']:\n",
    "        raise ValueError(\"Bu ID ile eşleşen video bulunamadı.\")\n",
    "    \n",
    "    video_data = response['items'][0]\n",
    "    duration = isodate.parse_duration(video_data['contentDetails']['duration']).total_seconds()\n",
    "    metadata = {\n",
    "        'video_id': video_id,\n",
    "        'title': video_data['snippet']['title'],\n",
    "        'description': video_data['snippet']['description'],\n",
    "        'tags': video_data['snippet'].get('tags', []),\n",
    "        'channel_title': video_data['snippet']['channelTitle'],\n",
    "        'channel_id': video_data['snippet']['channelId'],\n",
    "        'publish_date': video_data['snippet']['publishedAt'],\n",
    "        'thumbnail_url': video_data['snippet']['thumbnails']['high']['url'],\n",
    "        'view_count': int(video_data['statistics'].get('viewCount', 0)),\n",
    "        'like_count': int(video_data['statistics'].get('likeCount', 0)),\n",
    "        'comment_count': int(video_data['statistics'].get('commentCount', 0)),\n",
    "        'duration_seconds': duration,\n",
    "        'licensed_content': video_data['contentDetails'].get('licensedContent', False),\n",
    "        'privacy_status': video_data['status']['privacyStatus'],\n",
    "        'embeddable': video_data['status']['embeddable'],\n",
    "        'public_stats_viewable': video_data['status']['publicStatsViewable'],\n",
    "        'category_id': video_data['snippet'].get('categoryId', None),\n",
    "        'topic_categories': video_data.get('topicDetails', {}).get('topicCategories', []),\n",
    "        'live_broadcast': video_data.get('liveStreamingDetails', {}).get('actualStartTime', None),\n",
    "        'default_language': video_data['snippet'].get('defaultLanguage')\n",
    "    }\n",
    "    return metadata\n",
    "\n",
    "def truncate_text(text, max_tokens=MAX_TOKENS):\n",
    "    \"\"\"Yorum metnini belirli bir token uzunluğuna kısaltır.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Metni ön işleme tabi tutar.\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # URL'leri kaldır\n",
    "    text = re.sub(r'@\\w+', '', text)     # Kullanıcı etiketlerini kaldır\n",
    "    text = re.sub(r'#\\w+', '', text)     # Hashtag'leri kaldır\n",
    "    text = re.sub(r'\\s+', ' ', text)     # Fazla boşlukları kaldır\n",
    "    return text.strip()\n",
    "\n",
    "def get_comment_sentiments(video_id, youtube, max_comments=1000):\n",
    "    \"\"\"Yorumları analiz ederek duygusal dağılım, anahtar kelimeler ve en aktif kullanıcıları bulur.\"\"\"\n",
    "    comments = []\n",
    "    sentiment_counts = Counter()\n",
    "    emotion_counts = Counter()\n",
    "    active_users = Counter()\n",
    "    key_phrases = Counter()\n",
    "    comment_times = []\n",
    "    comment_lengths = []\n",
    "    comment_likes = []\n",
    "    user_interactions = []\n",
    "    \n",
    "    stop_words = set(stopwords.words('english') + stopwords.words('turkish'))\n",
    "\n",
    "    next_page_token = None\n",
    "\n",
    "    print(\"Yorumlar indiriliyor ve analiz ediliyor...\")\n",
    "    pbar = tqdm(total=max_comments)\n",
    "    while len(comments) < max_comments:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=min(max_comments - len(comments), 100),\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            comment_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = comment_snippet['textDisplay']\n",
    "            truncated_text = truncate_text(comment_text)\n",
    "            preprocessed_text = preprocess_text(truncated_text)\n",
    "            author = comment_snippet['authorDisplayName']\n",
    "            like_count = comment_snippet['likeCount']\n",
    "            published_at = comment_snippet['publishedAt']\n",
    "            published_at_dt = datetime.datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            comment_times.append(published_at_dt)\n",
    "            comment_lengths.append(len(preprocessed_text.split()))\n",
    "            comment_likes.append(like_count)\n",
    "\n",
    "            # Duygu analizi\n",
    "            sentiment = sentiment_analyzer(preprocessed_text)[0]\n",
    "            sentiment_counts[sentiment['label']] += 1\n",
    "\n",
    "            # Duygu yoğunluğu\n",
    "            emotions = emotion_analyzer(preprocessed_text)[0]\n",
    "            dominant_emotion = max(emotions, key=lambda x: x['score'])\n",
    "            emotion_counts[dominant_emotion['label']] += 1\n",
    "\n",
    "            # Anahtar kelime analizi\n",
    "            words = word_tokenize(preprocessed_text)\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                if word.isalpha() and word not in stop_words and len(word) > 3:\n",
    "                    key_phrases[word] += 1\n",
    "\n",
    "            # Aktif kullanıcılar\n",
    "            active_users[author] += 1\n",
    "\n",
    "            # Kullanıcı etkileşimleri (yanıtlar)\n",
    "            if 'replies' in item:\n",
    "                for reply in item['replies']['comments']:\n",
    "                    reply_snippet = reply['snippet']\n",
    "                    replier = reply_snippet['authorDisplayName']\n",
    "                    user_interactions.append((author, replier))\n",
    "\n",
    "            # Yorum kaydı\n",
    "            comments.append({\n",
    "                'comment_text': comment_text,\n",
    "                'author': author,\n",
    "                'like_count': like_count,\n",
    "                'published_at': published_at,\n",
    "                'sentiment': sentiment['label'],\n",
    "                'emotion': dominant_emotion['label']\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    pbar.close()\n",
    "\n",
    "    # En sık kullanılan 20 anahtar kelime\n",
    "    top_keywords = key_phrases.most_common(20)\n",
    "    # En aktif 10 kullanıcı\n",
    "    top_active_users = active_users.most_common(10)\n",
    "\n",
    "    return comments, dict(sentiment_counts), dict(emotion_counts), top_keywords, top_active_users, comment_times, comment_lengths, comment_likes, user_interactions\n",
    "\n",
    "def generate_wordcloud(key_phrases):\n",
    "    \"\"\"Anahtar kelimelerin kelime bulutunu oluşturur.\"\"\"\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(key_phrases))\n",
    "    plt.figure(figsize=(15, 7.5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('wordcloud.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_sentiment_distribution(sentiment_counts):\n",
    "    \"\"\"Duygu dağılımını görselleştirir.\"\"\"\n",
    "    labels = list(sentiment_counts.keys())\n",
    "    sizes = list(sentiment_counts.values())\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.barplot(x=labels, y=sizes)\n",
    "    plt.title('Duygu Dağılımı')\n",
    "    plt.xlabel('Duygu')\n",
    "    plt.ylabel('Sayı')\n",
    "    plt.savefig('sentiment_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_emotion_distribution(emotion_counts):\n",
    "    \"\"\"Duygu yoğunluğu dağılımını görselleştirir.\"\"\"\n",
    "    labels = list(emotion_counts.keys())\n",
    "    sizes = list(emotion_counts.values())\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=labels, y=sizes)\n",
    "    plt.title('Duygu Yoğunluğu Dağılımı')\n",
    "    plt.xlabel('Duygu')\n",
    "    plt.ylabel('Sayı')\n",
    "    plt.savefig('emotion_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_comment_timeline(comment_times):\n",
    "    \"\"\"Yorumların zaman içindeki dağılımını görselleştirir.\"\"\"\n",
    "    df = pd.DataFrame({'timestamp': comment_times})\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df['count'] = 1\n",
    "    df = df.resample('D').sum()\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    df['count'].plot()\n",
    "    plt.title('Zaman İçinde Yorum Dağılımı')\n",
    "    plt.xlabel('Tarih')\n",
    "    plt.ylabel('Yorum Sayısı')\n",
    "    plt.savefig('comment_timeline.png')\n",
    "    plt.close()\n",
    "\n",
    "def perform_topic_modeling(comments):\n",
    "    \"\"\"LDA ile konu modellemesi yapar.\"\"\"\n",
    "    print(\"Konu modellemesi yapılıyor...\")\n",
    "    texts = [preprocess_text(truncate_text(comment['comment_text'])) for comment in comments]\n",
    "    stop_words = set(stopwords.words('english') + stopwords.words('turkish'))\n",
    "    texts_tokenized = [[word for word in word_tokenize(text.lower()) if word.isalpha() and word not in stop_words] for text in texts]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts_tokenized)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n",
    "\n",
    "    lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    topics_data = []\n",
    "    for topic in topics:\n",
    "        topics_data.append({'topic_id': topic[0], 'terms': topic[1]})\n",
    "\n",
    "    # Sonuçları kaydet\n",
    "    with open('topic_modeling.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(topics_data, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Konu modelleme sonuçları 'topic_modeling.json' dosyasına kaydedildi.\")\n",
    "\n",
    "def analyze_top_comments(comments):\n",
    "    \"\"\"En çok beğenilen yorumları analiz eder.\"\"\"\n",
    "    print(\"En beğenilen yorumlar analiz ediliyor...\")\n",
    "    top_comments = sorted(comments, key=lambda x: x['like_count'], reverse=True)[:10]\n",
    "    with open('top_comments.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(top_comments, f, ensure_ascii=False, indent=4)\n",
    "    print(\"En beğenilen yorumlar 'top_comments.json' dosyasına kaydedildi.\")\n",
    "\n",
    "def analyze_comment_lengths(comment_lengths):\n",
    "    \"\"\"Yorum uzunluğu istatistiklerini analiz eder.\"\"\"\n",
    "    print(\"Yorum uzunluğu istatistikleri analiz ediliyor...\")\n",
    "    average_length = sum(comment_lengths) / len(comment_lengths)\n",
    "    median_length = sorted(comment_lengths)[len(comment_lengths) // 2]\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.histplot(comment_lengths, bins=30, kde=True)\n",
    "    plt.title('Yorum Uzunluğu Dağılımı')\n",
    "    plt.xlabel('Kelime Sayısı')\n",
    "    plt.ylabel('Yorum Sayısı')\n",
    "    plt.savefig('comment_length_distribution.png')\n",
    "    plt.close()\n",
    "    # Sonuçları kaydet\n",
    "    length_stats = {\n",
    "        'average_length': average_length,\n",
    "        'median_length': median_length\n",
    "    }\n",
    "    with open('comment_length_stats.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(length_stats, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Yorum uzunluğu istatistikleri 'comment_length_stats.json' dosyasına kaydedildi.\")\n",
    "\n",
    "def perform_clustering(comments):\n",
    "    \"\"\"Yorumları kümeleme analizi yapar.\"\"\"\n",
    "    print(\"Kümeleme analizi yapılıyor...\")\n",
    "    texts = [preprocess_text(truncate_text(comment['comment_text'])) for comment in comments]\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "\n",
    "    k = 5  # Küme sayısı\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Sonuçları kaydet\n",
    "    clustered_comments = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        clustered_comments[int(label)].append(comments[idx])\n",
    "\n",
    "    with open('clustering_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(clustered_comments, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Kümeleme sonuçları 'clustering_results.json' dosyasına kaydedildi.\")\n",
    "\n",
    "def analyze_user_interactions(user_interactions):\n",
    "    \"\"\"Kullanıcılar arasındaki etkileşimleri analiz eder.\"\"\"\n",
    "    print(\"Kullanıcı etkileşim ağı oluşturuluyor...\")\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(user_interactions)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    pos = nx.spring_layout(G, k=0.5)\n",
    "    nx.draw(G, pos, node_size=50, with_labels=False)\n",
    "    plt.savefig('user_interaction_network.png')\n",
    "    plt.close()\n",
    "    print(\"Kullanıcı etkileşim ağı 'user_interaction_network.png' olarak kaydedildi.\")\n",
    "\n",
    "def plot_sentiment_trends(comments):\n",
    "    \"\"\"Duygu eğilimlerini zaman içinde görselleştirir.\"\"\"\n",
    "    print(\"Duygu trendleri analiz ediliyor...\")\n",
    "    df = pd.DataFrame(comments)\n",
    "    df['published_at'] = pd.to_datetime(df['published_at'])\n",
    "    df.set_index('published_at', inplace=True)\n",
    "    sentiment_trends = df.resample('D')['sentiment'].value_counts().unstack().fillna(0)\n",
    "    sentiment_trends.plot(kind='line', figsize=(15, 7))\n",
    "    plt.title('Zaman İçinde Duygu Trendleri')\n",
    "    plt.xlabel('Tarih')\n",
    "    plt.ylabel('Yorum Sayısı')\n",
    "    plt.savefig('sentiment_trends.png')\n",
    "    plt.close()\n",
    "    print(\"Duygu trend grafiği 'sentiment_trends.png' olarak kaydedildi.\")\n",
    "\n",
    "def save_metadata_to_json(metadata):\n",
    "    \"\"\"Video metadata bilgilerini JSON dosyasına kaydeder.\"\"\"\n",
    "    with open('video_metadata.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(metadata, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def save_sentiment_analysis_to_json(comments, sentiment_counts, emotion_counts, top_keywords, top_active_users):\n",
    "    \"\"\"Sentiment analiz sonuçlarını JSON dosyasına kaydeder.\"\"\"\n",
    "    data = {\n",
    "        'comments': comments,\n",
    "        'sentiment_distribution': sentiment_counts,\n",
    "        'emotion_distribution': emotion_counts,\n",
    "        'top_keywords': top_keywords,\n",
    "        'top_active_users': top_active_users\n",
    "    }\n",
    "    with open('sentiment_analysis.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    youtube_url = input(\"YouTube Video URL'sini girin: \")\n",
    "    youtube = get_youtube_service()\n",
    "    video_id = extract_video_id(youtube_url)\n",
    "    \n",
    "    # 1. Video Metadata Analizi\n",
    "    print(\"Video metadata bilgileri alınıyor...\")\n",
    "    metadata = get_video_metadata(video_id, youtube)\n",
    "    save_metadata_to_json(metadata)\n",
    "    print(\"Video metadata bilgileri 'video_metadata.json' dosyasına kaydedildi.\")\n",
    "\n",
    "    # 2. Sentiment Analizi ve Ekstra Analizler\n",
    "    comments, sentiment_counts, emotion_counts, top_keywords, top_active_users, comment_times, comment_lengths, comment_likes, user_interactions = get_comment_sentiments(video_id, youtube, max_comments=1000)\n",
    "    save_sentiment_analysis_to_json(comments, sentiment_counts, emotion_counts, top_keywords, top_active_users)\n",
    "    print(\"Sentiment analiz sonuçları 'sentiment_analysis.json' dosyasına kaydedildi.\")\n",
    "\n",
    "    # 3. Görselleştirmeler ve Ek Analizler\n",
    "    generate_wordcloud(top_keywords)\n",
    "    print(\"Kelime bulutu 'wordcloud.png' olarak kaydedildi.\")\n",
    "\n",
    "    plot_sentiment_distribution(sentiment_counts)\n",
    "    print(\"Duygu dağılım grafiği 'sentiment_distribution.png' olarak kaydedildi.\")\n",
    "\n",
    "    plot_emotion_distribution(emotion_counts)\n",
    "    print(\"Duygu yoğunluğu grafiği 'emotion_distribution.png' olarak kaydedildi.\")\n",
    "\n",
    "    plot_comment_timeline(comment_times)\n",
    "    print(\"Yorum zaman çizelgesi 'comment_timeline.png' olarak kaydedildi.\")\n",
    "\n",
    "    perform_topic_modeling(comments)\n",
    "\n",
    "    analyze_top_comments(comments)\n",
    "\n",
    "    analyze_comment_lengths(comment_lengths)\n",
    "\n",
    "    perform_clustering(comments)\n",
    "\n",
    "    analyze_user_interactions(user_interactions)\n",
    "\n",
    "    plot_sentiment_trends(comments)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
